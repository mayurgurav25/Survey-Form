{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "h9TL-odR35CQ",
        "PepMNbHe5YqG",
        "QP-fwulq5Ua6",
        "_u4T4UWj8LdX",
        "QX84oUoB-Fc-",
        "uWDX8YN7EH1y",
        "xQ7FkNbfKaHj",
        "g_K-V0vrKpB6",
        "JyYZaiLrKqJr"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayurgurav25/Survey-Form/blob/main/Viva_Codes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q. Take the text of your choice (5 to 6 sentences) and perform the following operations"
      ],
      "metadata": {
        "id": "h9TL-odR35CQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FQTIEhK3xGY",
        "outputId": "1f7b3296-6135-459e-a545-e91db990aa80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.9/dist-packages (3.5.2)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy) (67.6.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "2023-04-23 17:35:10.338693: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "# Install libraries\n",
        "!pip install nltk\n",
        "\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.util import ngrams\n",
        "import string\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7iiABky5E-Z",
        "outputId": "86f12bf2-bd16-4aeb-97a1-a530833a83ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "text = \"The quick brown fox jumps over the lazy dog. Mary had a little lamb, its fleece was white as snow. She sells seashells by the seashore. The early bird catches the worm. I scream, you scream, we all scream for ice cream. There are 10 types of people in the world: those who understand binary with 0s and 1s, and those who don't.\""
      ],
      "metadata": {
        "id": "8KFQ8kBc5CBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1. a) Sentence tokenization b) Word tokenization c) Stemming**\n",
        "In this code, we first install the NLTK library (if it's not already installed) and import the necessary modules: sent_tokenize for sentence tokenization, word_tokenize for word tokenization, and PorterStemmer for stemming.\n",
        "\n",
        "We then input the text and perform sentence tokenization using the sent_tokenize() function, which splits the text into a list of sentences. We print each sentence using a for loop.\n",
        "\n",
        "Next, we perform word tokenization using the word_tokenize() function, which splits the text into a list of words. We print each word using a for loop.\n",
        "\n",
        "Finally, we perform stemming using the PorterStemmer() function, which reduces each word to its root or stem. We apply the stemming to each word using a list comprehension, and print each stemmed word using a for loop."
      ],
      "metadata": {
        "id": "PepMNbHe5YqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentences:\")\n",
        "for sentence in sentences:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kSvihgk4f1Q",
        "outputId": "d7b0fce6-e537-415b-c5de-f140abcf2ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            "The quick brown fox jumps over the lazy dog.\n",
            "Mary had a little lamb, its fleece was white as snow.\n",
            "She sells seashells by the seashore.\n",
            "The early bird catches the worm.\n",
            "I scream, you scream, we all scream for ice cream.\n",
            "There are 10 types of people in the world: those who understand binary with 0s and 1s, and those who don't.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word tokenization\n",
        "words = word_tokenize(text)\n",
        "print(\"\\nWords:\")\n",
        "for word in words:\n",
        "  print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIMBSQVu4h8K",
        "outputId": "05d50cc7-ead8-433a-f047-ee70b91fa9d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Words:\n",
            "The\n",
            "quick\n",
            "brown\n",
            "fox\n",
            "jumps\n",
            "over\n",
            "the\n",
            "lazy\n",
            "dog\n",
            ".\n",
            "Mary\n",
            "had\n",
            "a\n",
            "little\n",
            "lamb\n",
            ",\n",
            "its\n",
            "fleece\n",
            "was\n",
            "white\n",
            "as\n",
            "snow\n",
            ".\n",
            "She\n",
            "sells\n",
            "seashells\n",
            "by\n",
            "the\n",
            "seashore\n",
            ".\n",
            "The\n",
            "early\n",
            "bird\n",
            "catches\n",
            "the\n",
            "worm\n",
            ".\n",
            "I\n",
            "scream\n",
            ",\n",
            "you\n",
            "scream\n",
            ",\n",
            "we\n",
            "all\n",
            "scream\n",
            "for\n",
            "ice\n",
            "cream\n",
            ".\n",
            "There\n",
            "are\n",
            "10\n",
            "types\n",
            "of\n",
            "people\n",
            "in\n",
            "the\n",
            "world\n",
            ":\n",
            "those\n",
            "who\n",
            "understand\n",
            "binary\n",
            "with\n",
            "0s\n",
            "and\n",
            "1s\n",
            ",\n",
            "and\n",
            "those\n",
            "who\n",
            "do\n",
            "n't\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(\"\\nStemmed words:\")\n",
        "for word in stemmed_words:\n",
        "  print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji59sVxr4m7c",
        "outputId": "864311c5-46d8-471e-ed3b-4b284de84223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemmed words:\n",
            "the\n",
            "quick\n",
            "brown\n",
            "fox\n",
            "jump\n",
            "over\n",
            "the\n",
            "lazi\n",
            "dog\n",
            ".\n",
            "mari\n",
            "had\n",
            "a\n",
            "littl\n",
            "lamb\n",
            ",\n",
            "it\n",
            "fleec\n",
            "wa\n",
            "white\n",
            "as\n",
            "snow\n",
            ".\n",
            "she\n",
            "sell\n",
            "seashel\n",
            "by\n",
            "the\n",
            "seashor\n",
            ".\n",
            "the\n",
            "earli\n",
            "bird\n",
            "catch\n",
            "the\n",
            "worm\n",
            ".\n",
            "i\n",
            "scream\n",
            ",\n",
            "you\n",
            "scream\n",
            ",\n",
            "we\n",
            "all\n",
            "scream\n",
            "for\n",
            "ice\n",
            "cream\n",
            ".\n",
            "there\n",
            "are\n",
            "10\n",
            "type\n",
            "of\n",
            "peopl\n",
            "in\n",
            "the\n",
            "world\n",
            ":\n",
            "those\n",
            "who\n",
            "understand\n",
            "binari\n",
            "with\n",
            "0s\n",
            "and\n",
            "1s\n",
            ",\n",
            "and\n",
            "those\n",
            "who\n",
            "do\n",
            "n't\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **d) Remove stop words and punctuation e) Convert all text into lower case**\n",
        "\n",
        "This line converts the text to lower case using the lower() function.\n",
        "\n",
        "This line tokenizes the text into words using the word_tokenize() function.\n",
        "These lines remove stop words and punctuation from the tokenized words using list comprehensions. We first create a set of stop words using the set() function and the stopwords corpus from NLTK. We then create a set of punctuation using the set() function and the string module. Finally, we create a new list filtered_words that contains only the words from words that are not in stop_words and punctuation"
      ],
      "metadata": {
        "id": "QP-fwulq5Ua6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to lower case\n",
        "text = text.lower()\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlSTi0fB6idh",
        "outputId": "0f3c1d00-c58b-47d9-e469-9027fc5d8efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the quick brown fox jumps over the lazy dog. mary had a little lamb, its fleece was white as snow. she sells seashells by the seashore. the early bird catches the worm. i scream, you scream, we all scream for ice cream. there are 10 types of people in the world: those who understand binary with 0s and 1s, and those who don't.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuations = set(string.punctuation)\n",
        "filtered_words_lem = [word for word in words if word not in stop_words]\n",
        "filtered_words = [word for word in words if word not in stop_words and word not in punctuations]\n",
        "\n",
        "# Print filtered words\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxKgVUi6AEvC",
        "outputId": "4f4a70c6-7919-467b-e5ee-2889b5943908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'Mary', 'little', 'lamb', 'fleece', 'white', 'snow', 'She', 'sells', 'seashells', 'seashore', 'The', 'early', 'bird', 'catches', 'worm', 'I', 'scream', 'scream', 'scream', 'ice', 'cream', 'There', '10', 'types', 'people', 'world', 'understand', 'binary', '0s', '1s', \"n't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **f) Remove numbers from text**\n",
        "\n",
        "In this code, we first import the re module, which provides support for regular expressions in Python. We then input the text and assign it to the variable text.\n",
        "\n",
        "We use the re.sub() function to substitute all numbers in the text with an empty string. The regular expression pattern r'\\d+' matches one or more digits (0-9) in the text.\n",
        "\n",
        "Finally, we print the text without numbers using the print() function."
      ],
      "metadata": {
        "id": "_u4T4UWj8LdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove numbers\n",
        "text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "# Print text without numbers\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i10OwCc81hl",
        "outputId": "aa50ac75-8d18-48a8-dfd9-6effa55d02c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the quick brown fox jumps over the lazy dog. mary had a little lamb, its fleece was white as snow. she sells seashells by the seashore. the early bird catches the worm. i scream, you scream, we all scream for ice cream. there are  types of people in the world: those who understand binary with s and s, and those who don't.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **g) Apply Lemmatization h) POS tagging i) Extract N-grams**\n",
        "\n",
        "In this code, we first import necessary libraries from NLTK such as stopwords, word_tokenize, WordNetLemmatizer, pos_tag, and ngrams. We then download the necessary resources using the nltk.download() function.\n",
        "\n",
        "We input the text and assign it to the variable text.\n",
        "\n",
        "We then tokenize the words using word_tokenize() function and remove stop words using list comprehension.\n",
        "\n",
        "We then use the WordNetLemmatizer() function to lemmatize the words using list comprehension.\n",
        "\n",
        "We use the pos_tag() function to perform POS tagging on the lemmatized words.\n",
        "\n",
        "Finally, we use the ngrams() function to extract N-grams from the lemmatized words. We specify the value of n to define the number of words in a sequence.\n",
        "\n",
        "The results of lemmatization, POS tagging, and N-grams are printed to the console using the print() function."
      ],
      "metadata": {
        "id": "QX84oUoB-Fc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words_lem]\n",
        "\n",
        "# Print results\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdAooCEf_HnW",
        "outputId": "43722ec1-d299-47d1-c486-d1cf791b676c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized words: ['The', 'quick', 'brown', 'fox', 'jump', 'lazy', 'dog', '.', 'Mary', 'little', 'lamb', ',', 'fleece', 'white', 'snow', '.', 'She', 'sell', 'seashell', 'seashore', '.', 'The', 'early', 'bird', 'catch', 'worm', '.', 'I', 'scream', ',', 'scream', ',', 'scream', 'ice', 'cream', '.', 'There', '10', 'type', 'people', 'world', ':', 'understand', 'binary', '0', '1', ',', \"n't\", '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# POS tagging\n",
        "pos_tags = pos_tag(lemmatized_words)\n",
        "\n",
        "# Print results\n",
        "print(\"POS tags:\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIgSXLDgAcGA",
        "outputId": "0cb692c5-5bfe-4797-9fad-a4b5d07d8d66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tags: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'JJ'), ('jump', 'NN'), ('lazy', 'NN'), ('dog', 'NN'), ('.', '.'), ('Mary', 'NNP'), ('little', 'JJ'), ('lamb', 'NN'), (',', ','), ('fleece', 'JJ'), ('white', 'JJ'), ('snow', 'NN'), ('.', '.'), ('She', 'PRP'), ('sell', 'VBP'), ('seashell', 'RB'), ('seashore', 'RB'), ('.', '.'), ('The', 'DT'), ('early', 'JJ'), ('bird', 'NN'), ('catch', 'NN'), ('worm', 'NN'), ('.', '.'), ('I', 'PRP'), ('scream', 'VBP'), (',', ','), ('scream', 'NN'), (',', ','), ('scream', 'NN'), ('ice', 'NN'), ('cream', 'NN'), ('.', '.'), ('There', 'EX'), ('10', 'CD'), ('type', 'NN'), ('people', 'NNS'), ('world', 'NN'), (':', ':'), ('understand', 'JJ'), ('binary', 'JJ'), ('0', 'CD'), ('1', 'CD'), (',', ','), (\"n't\", 'RB'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract N-grams\n",
        "n = 2 # Number of words in a sequence\n",
        "n_grams = ngrams(lemmatized_words, n)\n",
        "\n",
        "# Print results\n",
        "print(\"N-grams:\", list(n_grams))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yUuJUBZAeWn",
        "outputId": "17bfface-f25d-4a4e-ecc7-83e2aca7ac4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N-grams: [('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jump'), ('jump', 'lazy'), ('lazy', 'dog'), ('dog', '.'), ('.', 'Mary'), ('Mary', 'little'), ('little', 'lamb'), ('lamb', ','), (',', 'fleece'), ('fleece', 'white'), ('white', 'snow'), ('snow', '.'), ('.', 'She'), ('She', 'sell'), ('sell', 'seashell'), ('seashell', 'seashore'), ('seashore', '.'), ('.', 'The'), ('The', 'early'), ('early', 'bird'), ('bird', 'catch'), ('catch', 'worm'), ('worm', '.'), ('.', 'I'), ('I', 'scream'), ('scream', ','), (',', 'scream'), ('scream', ','), (',', 'scream'), ('scream', 'ice'), ('ice', 'cream'), ('cream', '.'), ('.', 'There'), ('There', '10'), ('10', 'type'), ('type', 'people'), ('people', 'world'), ('world', ':'), (':', 'understand'), ('understand', 'binary'), ('binary', '0'), ('0', '1'), ('1', ','), (',', \"n't\"), (\"n't\", '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Chunking**\n",
        "\n",
        "In this code, we first import necessary libraries from NLTK such as word_tokenize, pos_tag, and RegexpParser.\n",
        "\n",
        "We input the text and assign it to the variable text.\n",
        "\n",
        "We then tokenize the words using word_tokenize() function and perform POS tagging using pos_tag() function.\n",
        "\n",
        "We define the chunk grammar using regular expressions. Here, we define two rules - one for Noun Phrases (NP) and one for Verb Phrases (VP). The NP rule matches any combination of determiners, adjectives, and nouns, while the VP rule matches any verb followed by one or more NPs, PPs, or clauses.\n",
        "\n",
        "We create a RegexpParser() object using the chunk grammar.\n",
        "\n",
        "We then use the parse() function of the RegexpParser object to perform chunking on the POS-tagged words.\n",
        "\n",
        "We then loop through the chunks using subtrees() method and extract the Verb and Noun chunks.\n",
        "\n",
        "Finally, we print the extracted Verb and Noun chunks using the print() function."
      ],
      "metadata": {
        "id": "uWDX8YN7EH1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "# Input text\n",
        "text = \"John saw the cat on the mat and chased it.\"\n",
        "\n",
        "# Tokenize words\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "# Define chunk grammar\n",
        "chunk_grammar = r\"\"\"\n",
        "    NP: {<DT|PRP\\$>?<JJ>*<NN>} # Noun Phrase\n",
        "    VP: {<VB.*><NP|PP|CLAUSE>+$} # Verb Phrase\n",
        "\"\"\"\n",
        "\n",
        "# Create chunk parser\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "\n",
        "# Perform chunking\n",
        "chunks = chunk_parser.parse(pos_tags)\n",
        "\n",
        "# Extract Verb and Noun chunks\n",
        "verb_chunks = []\n",
        "noun_chunks = []\n",
        "\n",
        "for chunk in chunks.subtrees():\n",
        "    if chunk.label() == 'VP':\n",
        "        verb_chunks.append(' '.join([word for word, tag in chunk.leaves()]))\n",
        "    elif chunk.label() == 'NP':\n",
        "        noun_chunks.append(' '.join([word for word, tag in chunk.leaves()]))\n",
        "\n",
        "# Print results\n",
        "print(\"Verb chunks:\", verb_chunks)\n",
        "print(\"Noun chunks:\", noun_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNPa21zOENOM",
        "outputId": "947d2841-fd63-4efb-ee51-55422801a1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verb chunks: []\n",
            "Noun chunks: ['the cat', 'the mat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Date Formating**\n",
        "\n",
        "In this code, we first import the re library to use regular expressions.\n",
        "\n",
        "We input the text and assign it to the variable text.\n",
        "\n",
        "We then define three regex patterns for different date formats: date_pattern_1 for mm/dd/yyyy format, date_pattern_2 for yyyy-mm-dd format, and date_pattern_3 for Day, Month ddth yyyy format.\n",
        "\n",
        "We then create an empty list dates to store extracted dates.\n",
        "\n",
        "We loop through each regex pattern and extract dates using the findall() function of the re library. We use the extend() method to add the extracted dates to the dates list.\n",
        "\n",
        "Finally, we print the extracted dates using the print() function.\n",
        "\n",
        "You can modify the regex patterns to match different date formats according to your needs. Additionally, you can also use this code to extract other patterns from text using regular expressions."
      ],
      "metadata": {
        "id": "xQ7FkNbfKaHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Input text\n",
        "text = \"The deadline for the project is 12/31/2023. The event starts at 2023-05-01. The meeting is on Tuesday, May 10th 2022.\"\n",
        "\n",
        "# Define regex patterns for different date formats\n",
        "date_pattern_1 = r\"\\d{1,2}/\\d{1,2}/\\d{4}\" # mm/dd/yyyy\n",
        "date_pattern_2 = r\"\\d{4}-\\d{2}-\\d{2}\" # yyyy-mm-dd\n",
        "date_pattern_3 = r\"[A-Za-z]+,\\s[A-Za-z]+\\s\\d{1,2}(st|nd|rd|th)?\\s\\d{4}\" # Day, Month ddth yyyy\n",
        "\n",
        "# Extract dates using regex patterns\n",
        "dates = []\n",
        "for pattern in [date_pattern_1, date_pattern_2, date_pattern_3]:\n",
        "    dates.extend(re.findall(pattern, text))\n",
        "\n",
        "# Print extracted dates\n",
        "print(\"Dates found in text:\", dates)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EaettnjKnBz",
        "outputId": "0480910e-f8f8-49da-be17-34b001f07163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dates found in text: ['12/31/2023', '2023-05-01', 'th']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NER Extraction**\n",
        "\n",
        "In this code, we first install the spacy library and download the en_core_web_sm model by running the first two lines.\n",
        "\n",
        "We then import the spacy library.\n",
        "\n",
        "We load the English NLP model using the spacy.load() function and assign it to the variable nlp.\n",
        "\n",
        "We input the text and assign it to the variable text.\n",
        "\n",
        "We process the text using the nlp() method of the nlp model and assign it to the variable doc.\n",
        "\n",
        "We then loop through the named entities in doc using the doc.ents attribute and extract their text and label using the ent.text and ent.label_ attributes respectively. We store these tuples in a list entities.\n",
        "\n",
        "Finally, we print the extracted named entities using the print() function.\n",
        "\n",
        "You can modify the code to extract other types of named entities by modifying the ent.label_ attribute. You can find a list of named entity labels in spaCy's documentation."
      ],
      "metadata": {
        "id": "g_K-V0vrKpB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input text\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "\n",
        "# Process the text with the NLP model\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract named entities\n",
        "entities = []\n",
        "for ent in doc.ents:\n",
        "    entities.append((ent.text, ent.label_))\n",
        "\n",
        "# Print the named entities\n",
        "print(\"Named entities found in text:\", entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xm4RvplKpl-",
        "outputId": "0a53e464-6443-4bc6-be79-6c2e97063f00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named entities found in text: [('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Total no. of words.** \n",
        "Also Find \n",
        "\n",
        "a) Unique words in a given text \n",
        "\n",
        "b) The frequency of each word \n",
        "\n",
        "c) The word with the longest length \n",
        "\n",
        "d) The words that start with letter A or a.\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "In this code, we first import the string module to remove punctuations.\n",
        "\n",
        "We input the text and assign it to the variable text.\n",
        "\n",
        "We remove punctuations from the text using the translate() method of the str class and assign the result back to text.\n",
        "\n",
        "We convert the text to lowercase using the lower() method of the str class and assign the result back to text.\n",
        "\n",
        "We split the text into words using the split() method of the str class and assign the result to the list words.\n",
        "\n",
        "We find the total number of words in the text using the len() function and assign it to the variable total_words.\n",
        "\n",
        "We find unique words in the text using the set() function and assign the result to the set unique_words.\n",
        "\n",
        "We find the frequency of each word in the text using a dictionary word_freq. We loop through each word in words, check if the word is already in word_freq. If it is, we increment its frequency count. Otherwise, we add it to word_freq with a frequency count of 1.\n",
        "\n",
        "We find the word with the longest length using the max() function and assign it to the variable longest_word.\n",
        "\n",
        "We find words that start with the letter A or a using a list comprehension and assign the result to the list words_starting_with_a.\n",
        "\n",
        "Finally, we print the results of each operation using the print() function."
      ],
      "metadata": {
        "id": "JyYZaiLrKqJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# Input text\n",
        "text = \"The quick brown fox jumps over the lazy dog. The dog is not amused. The brown fox is quick and agile.\"\n",
        "\n",
        "# Remove punctuations\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Convert to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Split text into words\n",
        "words = text.split()\n",
        "\n",
        "# Total number of words\n",
        "total_words = len(words)\n",
        "print(\"Total number of words in the text:\", total_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcXeI3hQKqd_",
        "outputId": "1b432e04-bcde-4061-9cd5-26291bead0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of words in the text: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique words\n",
        "unique_words = set(words)\n",
        "print(\"Unique words in the text:\", unique_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p3j8zU0MV7M",
        "outputId": "e4bbd3c1-e082-4412-fa51-83c8c7fdc3fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words in the text: {'dog', 'the', 'not', 'and', 'jumps', 'over', 'lazy', 'agile', 'brown', 'fox', 'is', 'amused', 'quick'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency of each word\n",
        "word_freq = {}\n",
        "for word in words:\n",
        "    if word not in word_freq:\n",
        "        word_freq[word] = 1\n",
        "    else:\n",
        "        word_freq[word] += 1\n",
        "print(\"Frequency of each word:\", word_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFaij4jNMYER",
        "outputId": "c89bd268-8adc-4e89-9276-92fa764933ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency of each word: {'the': 4, 'quick': 2, 'brown': 2, 'fox': 2, 'jumps': 1, 'over': 1, 'lazy': 1, 'dog': 2, 'is': 2, 'not': 1, 'amused': 1, 'and': 1, 'agile': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word with longest length\n",
        "longest_word = max(words, key=len)\n",
        "print(\"Word with longest length:\", longest_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbQBAxzmMbkC",
        "outputId": "87c9006b-cec3-46b0-f58b-8e547ee134f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word with longest length: amused\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Words starting with A or a\n",
        "words_starting_with_a = [word for word in words if word.startswith(\"a\")]\n",
        "print(\"Words starting with A or a:\", words_starting_with_a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S05SfXn2Mczh",
        "outputId": "e81d1ee8-5639-4bf5-c71f-2188d60dccb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words starting with A or a: ['amused', 'and', 'agile']\n"
          ]
        }
      ]
    }
  ]
}